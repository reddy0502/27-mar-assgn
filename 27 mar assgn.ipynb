{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3361cf-d563-479a-a8ba-7867fcf92398",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "R-squared is a statistical measure used to evaluate the goodness of fit of a linear regression model. It represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in the model.\n",
    "\n",
    "R-squared is calculated as the ratio of the explained variance to the total variance:\n",
    "\n",
    "R-squared = Explained Variance / Total Variance\n",
    "\n",
    "The explained variance is the sum of the squared differences between the predicted values and the mean of the dependent variable, while the total variance is the sum of the squared differences between the actual values and the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f02ac-cbff-4bf6-9499-e89af5ed27ab",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables used in the model. It is used to evaluate the goodness of fit of a multiple linear regression model.\n",
    "\n",
    "While R-squared measures the proportion of variance in the dependent variable that is explained by the independent variable(s), adjusted R-squared adjusts this value for the number of independent variables in the model. It penalizes models that include too many independent variables that do not contribute significantly to the model's predictive power.\n",
    "\n",
    "Adjusted R-squared is calculated using the following formula:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b53d8-3685-4266-bfaf-76431f46dd7a",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "Adjusted R-squared is more appropriate than R-squared when dealing with multiple linear regression models that include multiple independent variables. This is because R-squared can give a misleading impression of the goodness of fit of the model when additional independent variables are added, even if those variables do not actually improve the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed94f3-a8d8-4ed2-ad10-b127c7e85684",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "RMSE (Root Mean Squared Error) is the square root of the mean of the squared differences between the predicted values and the actual values. It is calculated using the following formula:\n",
    "\n",
    "RMSE = sqrt(sum((y_pred - y_actual)^2) / n)\n",
    "\n",
    "MSE (Mean Squared Error) is the mean of the squared differences between the predicted values and the actual values. It is calculated using the following formula:\n",
    "\n",
    "MSE = sum((y_pred - y_actual)^2) / n\n",
    "\n",
    "MAE (Mean Absolute Error) is the mean of the absolute differences between the predicted values and the actual values. It is calculated using the following formula:\n",
    "\n",
    "MAE = sum(|y_pred - y_actual|) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9403b2-3ff5-4e97-9aab-db1414ec4c2c",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "Advantages of using RMSE:\n",
    "\n",
    "RMSE is a popular metric for regression analysis because it is sensitive to large errors. \n",
    "RMSE has the same units as the dependent variable, which can make it easier to interpret.\n",
    "\n",
    "Disadvantages of using RMSE:\n",
    "\n",
    "RMSE is highly sensitive to outliers, meaning that a few large errors in the predictions can significantly increase the RMSE value.\n",
    "RMSE may not be suitable for data with a wide range of values, as it is highly influenced by the scale of the dependent variable.\n",
    "\n",
    "Advantages of using MSE:\n",
    "\n",
    "MSE is a simple and intuitive metric that is easy to calculate.\n",
    "MSE is also sensitive to large errors, making it a useful metric for identifying models with poor performance.\n",
    "\n",
    "Disadvantages of using MSE:\n",
    "\n",
    "Like RMSE, MSE is highly sensitive to outliers, meaning that it may not be suitable for datasets with large outliers.\n",
    "MSE is not as easy to interpret as RMSE, as it is not in the same units as the dependent variable.\n",
    "\n",
    "Advantages of using MAE:\n",
    "\n",
    "MAE is less sensitive to outliers than RMSE and MSE, meaning that it can be a more robust metric for datasets with large outliers.\n",
    "MAE is easier to interpret than RMSE and MSE, as it is in the same units as the dependent variable.\n",
    "\n",
    "Disadvantages of using MAE:\n",
    "\n",
    "MAE is not sensitive to the size of errors, meaning that it may not effectively penalize a model for making large errors in its predictions.\n",
    "MAE may not be suitable for datasets with a wide range of values, as it may not effectively capture the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ba6a5-37e0-48c3-88f0-38d52cb239f1",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting and improve the model's generalization ability by adding a penalty term to the regression equation.\n",
    "\n",
    "This can be useful for feature selection, as it allows the model to automatically select the most important features by setting the corresponding coefficients to zero.\n",
    "\n",
    " Lasso regularization is more appropriate when the dataset has a large number of features, and some of them are irrelevant or redundant. In such cases, Lasso can effectively perform feature selection and improve the model's generalization ability. Ridge regularization is more suitable when the dataset has highly correlated features, and it is important to distribute the coefficients among them to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb78c91-78da-4dcf-91b0-1c241a39ff85",
   "metadata": {},
   "source": [
    "7ans:\n",
    "\n",
    "Regularized linear models, such as Ridge and Lasso regression, are used in machine learning to prevent overfitting, which occurs when a model learns the noise in the training data instead of the underlying patterns. Regularization adds a penalty term to the loss function that the model is trying to minimize, which constrains the model parameters and reduces their magnitude, making the model less likely to fit the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d808aba-794b-4ad0-b4e3-bcc6e09ffc38",
   "metadata": {},
   "source": [
    "8ans:\n",
    "\n",
    "Some of the limitations of regularized linear models include:\n",
    "\n",
    "Complexity: Regularized linear models can be complex to interpret, particularly when there are many features in the dataset.\n",
    "\n",
    "Tuning hyperparameters: Regularized linear models have hyperparameters that need to be tuned to obtain the best performance. This can be time-consuming and requires a lot of experimentation to find the optimal values of the hyperparameters.\n",
    "\n",
    "Non-linear relationships: Regularized linear models assume a linear relationship between the input features and the output variable\n",
    "\n",
    "Outliers: Regularized linear models are sensitive to outliers in the data, as the penalty term can disproportionately affect the coefficient estimates. Robust regression techniques, such as Huber regression, may be more appropriate in such cases.\n",
    "\n",
    "Small datasets: Regularized linear models may not be appropriate for small datasets with limited samples, as the penalty term can be too restrictive and result in underfitting the data. In such cases, simpler regression models, such as linear regression or polynomial regression, may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb0b96-528d-405b-98e1-542583a43237",
   "metadata": {},
   "outputs": [],
   "source": [
    "9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
